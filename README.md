```markdown
# Sign Translation Software

This project is a Sign Language Translation System that recognizes and translates American Sign Language (ASL) alphabet gestures into text using computer vision and deep learning. It leverages MediaPipe for hand landmark detection and a Convolutional Neural Network (CNN) for classification. A Tkinter-based Python application is developed for real-time sign recognition, and a mobile application extends the functionality with text-to-sign, audio-to-sign, and an ASL dictionary.

## ğŸš€ Features
- ğŸ¥ Real-time ASL alphabet recognition using computer vision.
- ğŸ–ï¸ MediaPipe-based hand landmark detection for efficient processing.
- ğŸ§  CNN-based classification model for gesture recognition.
- ğŸ–¥ï¸ Tkinter GUI application for user-friendly interaction.
- ğŸ“± Mobile application with:
  - ğŸ“ Text-to-sign translation
  - ğŸ™ï¸ Audio-to-sign conversion
  - ğŸ“– ASL dictionary for learning gestures.
- ğŸ“Š Hierarchical classification approach to improve accuracy.
- ğŸŒ Robustness under different lighting conditions and hand positioning.


## ğŸ“± Mobile Application
The mobile version of the application supports text-to-sign and audio-to-sign features.  
The source code can be found in the [`final-app/`]directory.

## ğŸ“Š Model Performance
The CNN model was trained on ASL alphabet gestures and evaluated using:
- âœ… Accuracy: 85.21%
- ğŸ¯ Precision: 87%
- ğŸ“ˆ Recall: 85%
- ğŸ”„ F1-score: 85%

Despite good performance under controlled conditions, the model faces challenges with varying lighting and background complexity.

## ğŸ—ï¸ Future Enhancements
ğŸ”¹ Expanding the dataset for better generalization.  
ğŸ”¹ Improving model accuracy with deeper architectures.  
ğŸ”¹ Extending support for full sign language words/sentences.  
ğŸ”¹ Optimizing for low-power mobile devices.  

